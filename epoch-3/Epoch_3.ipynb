{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Epoch 3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "iNeL51i8xAGk"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Onqg6s2l_eAn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Epoch 3"
      ]
    },
    {
      "metadata": {
        "id": "iNeL51i8xAGk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports and utility functions"
      ]
    },
    {
      "metadata": {
        "id": "y4x3GwQJxFMa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import types\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "fig = plt.figure()\n",
        "plt.style.use('ggplot')\n",
        "plt.rcParams['image.cmap'] = 'RdBu'\n",
        "\n",
        "import sklearn.datasets as datasets\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "\n",
        "def plot_decision_boundary(model, X, y, degree=1):\n",
        "    \"\"\"\n",
        "    Use this to plot the decision boundary of a trained model.\n",
        "    \"\"\"\n",
        "    grid_lim = np.array([[X[:,0].min(), X[:,0].max()], [X[:,1].min(), X[:,1].max()]])\n",
        "    xx, yy = np.mgrid[grid_lim[0,0]:grid_lim[0,1]:.01, \n",
        "                      grid_lim[1,0]:grid_lim[1,1]:.01]\n",
        "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "    \n",
        "    t = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    _poly = t.fit_transform(grid)\n",
        "    \n",
        "    probs = model.predict_proba(_poly)[:, 1].reshape(xx.shape)\n",
        "    \n",
        "    f, ax = plt.subplots(figsize=(8, 6))\n",
        "    contour = ax.contourf(xx, yy, probs, 25, cmap=\"RdBu\",\n",
        "                        vmin=0, vmax=1)\n",
        "    ax_c = f.colorbar(contour)\n",
        "    ax_c.set_label(\"$P(y = 1)$\")\n",
        "    ax_c.set_ticks([0, .25, .5, .75, 1])\n",
        "\n",
        "    ax.scatter(X[:,0], X[:, 1], c=y, s=100,\n",
        "             cmap=\"RdBu\", vmin=-.2, vmax=1.2,\n",
        "             edgecolor=\"white\", linewidth=1)\n",
        "\n",
        "    ax.set(aspect=\"equal\",\n",
        "           xlim=(grid_lim[0,0],grid_lim[0,1]), \n",
        "           ylim=(grid_lim[1,0],grid_lim[1,1]),\n",
        "           xlabel=\"$X_1$\", ylabel=\"$X_2$\")\n",
        "    plt.gcf().set_size_inches(21, 14)\n",
        "    return f, ax\n",
        "  \n",
        "def plot_decision_boundry_linear(model, X, y):\n",
        "  def predict_proba(self, X):\n",
        "    x = np.float64(model.predict(X) > 0.5).reshape(-1, 1)\n",
        "    x = np.hstack((1-x, x))\n",
        "    \n",
        "    return x\n",
        "  \n",
        "  model.predict_proba = types.MethodType(predict_proba, model)\n",
        "  \n",
        "  plot_decision_boundary(model, X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vITWJGHY_jFa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Quick Recap\n"
      ]
    },
    {
      "metadata": {
        "id": "sXFCv1fO_rgl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Linear Regression\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/2000/1*lPsG_BLstPpJi6MZekn7-g.png)\n",
        "\n",
        "Last time we talked about linear regression. \n",
        "\n",
        "We found the best line that best fits our points. \n",
        "\n",
        "We also found ways to deal with non-linear data by fitting a polynomial (still linear regression - it's linear in coefficients not in features). \n",
        "\n",
        "We found that it is a powerful method yet simple method for estimating our target variable, and check for correlations in the data."
      ]
    },
    {
      "metadata": {
        "id": "66SRSKdy_tcI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Overfitting\n",
        "\n",
        "![](https://cdnpythonmachinelearning.azureedge.net/wp-content/uploads/2017/09/Overfitting.png?x31195)\n",
        "\n",
        "We also dealt with underfitting and overfitting.\n",
        "\n",
        "**Reminder**: overfitting is when the test score is very low compared to the training score - i.e. the model \"memorizes\" the training data and can't generalize to new data.\n",
        "\n",
        "**Reminder**: underfitting is when the model is not powerful enough to capture the complexity of the data.\n",
        "\n",
        "We have to find the sweet spot in the middle."
      ]
    },
    {
      "metadata": {
        "id": "7NB0Ctej_wH4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation\n",
        "\n",
        "### Train / test split\n",
        "\n",
        "**Reminder**: the score on the training set tells us nothing of the model's performance. A better estimate of that is by splitting our data into a training set and a test set - training on the training set and evaluating on the test set.\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1600/1*-8_kogvwmL1H6ooN1A1tsQ.png =400x)\n",
        "\n",
        "### K-Fold Cross Validation\n",
        "\n",
        "**Reminder**: K-Fold cross validation is a method with which we split our training data into multiple folds to have an \"average\" test score (in case we were really unlucky) with the split.\n",
        "\n",
        "![](https://www.researchgate.net/profile/Juan_Buhagiar2/publication/322509110/figure/fig1/AS:583173118664704@1516050714606/An-example-of-a-10-fold-cross-validation-cro17.png)\n"
      ]
    },
    {
      "metadata": {
        "id": "5iM6U6juADlj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Classification Problems"
      ]
    },
    {
      "metadata": {
        "id": "4Rrr8gYYKvIb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If in a regression setting we had some features $X$ and some target values $y$, we had to make a model that predicts new $y$'s that are as close as possible to the original $y$'s. In that case, the target variable is **continous**. \n",
        "\n",
        "Classification problems are different. "
      ]
    },
    {
      "metadata": {
        "id": "ziwBMLsTxn_E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's consider this dataset of two blobs, and try to make some predictions on it."
      ]
    },
    {
      "metadata": {
        "id": "qJ3FCzGWxwkc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X, y = datasets.make_blobs(1000, centers=2, random_state=0)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=100)\n",
        "plt.gcf().set_size_inches(21, 14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Vm1NIflGsd1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "How about we use LinearRegression to classify our data. This time, our data has 2 features:\n",
        "\n",
        "$X = \\begin{bmatrix}\n",
        "x_1^{(1)} & x_1^{(2)} \\\\\n",
        "x_2^{(1)} & x_2^{(2)} \\\\\n",
        "x_3^{(1)} & x_3^{(2)} \\\\\n",
        "\\vdots & \\vdots \\\\\n",
        "x_n^{(1)} & x_n^{(2)}\\\\\n",
        "\\end{bmatrix}$\n",
        "\n",
        "And the target variable $y$ has only two values: ${\\{0, 1}\\}$. This time we fit a *hyper-plane* through our datapoints,  will represent it using colors.\n",
        "\n",
        "After we fitted our model, to get the classes of our points we could just say that if a point has a prediction less than $0.5$ we consider it to be of class 0, and if a point has a prediction of over $0.5$ then we consider it belonging to class 1. You can see that in the *plot_decision_boundry_linear()* function at the top of the page."
      ]
    },
    {
      "metadata": {
        "id": "atmP55PYGref",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "# Training LinearRegression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Plotting the decision boundry\n",
        "plot_decision_boundry_linear(model, X, y)\n",
        "\n",
        "# 3d plot of the hyper-plane\n",
        "fig = plt.figure()\n",
        "ax = fig.gca(projection='3d')\n",
        "\n",
        "ax.scatter(X[:, 0], X[:, 1], y, 'o', c=y)\n",
        "ax.view_init(30, 35)\n",
        "ax.hold(True)\n",
        "\n",
        "\n",
        "_x = np.linspace(np.min(X[:, 0]-0.5), np.max(X[:, 0]) + 0.5, 100).reshape(-1, 1)\n",
        "_y = np.linspace(np.min(X[:, 1]) - 0.5, np.max(X[:, 1]) + 0.5, 100).reshape(-1, 1)\n",
        "_X = np.hstack((_x, _y))\n",
        "_z = model.predict(_X).reshape(1, -1)\n",
        "\n",
        "ax.plot_surface(_x, _y, _z, alpha=0.4, rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n",
        "plt.gcf().set_size_inches(21, 14)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y6MMraSxMGN4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Question\n",
        "\n",
        "Looks preety correct. But why did we choose 0.5 as our threshold? Why not 0.8? or 0.3?"
      ]
    },
    {
      "metadata": {
        "id": "AzFU-GigAMYF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression\n",
        "\n",
        "You could say that we chose 0.5 because it reflects a probability. But in fact that is arbitrary, and our regression model is not in any way related to probabilities. A modification of the LinearRegression model is **LogisticRegression**.\n",
        "\n",
        "Logistic Regression builds upon the **sigmoid** function.\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/thumb/5/53/Sigmoid-function-2.svg/2000px-Sigmoid-function-2.svg.png)\n",
        "The sigmoid is a function that can **be interpreted as a cumulative probability distribution**.\n",
        "\n",
        "It's just a linear model with the **sigmoid** on top of it.\n",
        "\n",
        "![](https://www.saedsayad.com/images/LogReg_1.png)\n",
        "\n",
        "\n",
        "**Note:** This time we are not interested in minimizing the **least squares**, but rather the **categorical cross entropy**.  It's a special function designed for classifications. You can read more about it below.\n",
        "\n",
        "Categorical cross entropy:\n",
        "- https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/\n",
        "\n",
        "Further reading:\n",
        "- https://www.saedsayad.com/logistic_regression.htm\n",
        "- https://towardsdatascience.com/understanding-logistic-regression-9b02c2aec102\n",
        "- https://en.wikipedia.org/wiki/Sigmoid_function"
      ]
    },
    {
      "metadata": {
        "id": "L796OMxt0El5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's try to train LogisticRegression on our dataset."
      ]
    },
    {
      "metadata": {
        "id": "SMa2F5dozHFQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Training the model on the data.\n",
        "model.fit(X, y)\n",
        "\n",
        "# Use this function to plot the decision boundry of your model.\n",
        "plot_decision_boundary(model, X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kyJeqvgy0SP_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can actually see the sigmoid that is applied to the linear model. It goes from probability 0 to probability 1. The plot shows the probability that a point is red. What is the accuracy of our model?"
      ]
    },
    {
      "metadata": {
        "id": "xuYqUIRA0mKd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.score(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8WG7iKTb0fzG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## What did I do wrong?"
      ]
    },
    {
      "metadata": {
        "id": "KO31JRfQ0vPz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The accuracy reported is on the training data. Which tells us **nothing** about or model. \n",
        "\n",
        "# Coding Challenge\n",
        "Fix the above code such that we can calculate the test accuracy."
      ]
    },
    {
      "metadata": {
        "id": "vkWt_cn50jjA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# TODO Split the data into train and test and calculate the test error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z-jWo_C4fB6P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://memegenerator.net/img/instances/74867847/then-he-separated-the-light-from-the-darkness-the-first-logistic-regression-model.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "InBbVITd1itg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Linear Separability\n",
        "\n",
        "Let's try to get fancy, and try to fit logistic regression on the moons dataset."
      ]
    },
    {
      "metadata": {
        "id": "fkhWQnnB1oQu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X, y = datasets.make_moons(1000, noise=0.2, random_state=0)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=100)\n",
        "plt.gcf().set_size_inches(21, 14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xImi3JXW18ua",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Same thing as before, splitting the data and training.\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "print(\"Accuracy\", model.score(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vnrj-Sv82Scg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "~85% accuracy. That's actually not bad. Let's plot the decision boundry."
      ]
    },
    {
      "metadata": {
        "id": "1kbaBJEQ2atR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_decision_boundary(model, X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "akqTTWXh4TDM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In fact, logistic regression does a poor job at classifying these points, because they are not **Lineary Separable**. That just means that there's no straight line that can separate the points. \n",
        "\n",
        "\n",
        "Further Reading:\n",
        "\n",
        "- https://en.wikipedia.org/wiki/Linear_separability"
      ]
    },
    {
      "metadata": {
        "id": "BBGRKq0X4yYZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Polynomial Features\n",
        "\n",
        "Since logistic regression is a **linear model**, we could try to apply PolynomialFeatures to it, as before. \n",
        "\n",
        "The graph below looks really close to the decision boundry for our data. It's a polynomial of degree 3.\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Polynomialdeg3.svg/1200px-Polynomialdeg3.svg.png)"
      ]
    },
    {
      "metadata": {
        "id": "lfBBE6Ja4wpe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# degree = 3, because that looks the most appropriate\n",
        "t = PolynomialFeatures(degree=3, include_bias=False)\n",
        "X_poly = t.fit_transform(X)\n",
        "\n",
        "# train / test split, as before; we want to be rigorous.\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_poly, y)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "print(\"Accuracy\", model.score(x_test, y_test))\n",
        "\n",
        "plot_decision_boundary(model, X, y, degree=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sr92Pj7q6JRF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Accuracy ~96%. A much better improvement. \n",
        "\n",
        "## Homework\n",
        "Of course, we could try some higher degrees with some regularization (LogisticRegression has the parameter **C** to control the power of regularization), as before, but **it's left as an exercise to the reader**\n",
        "\n",
        "Check out the documentation on LogisticRegression:\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "\n",
        "\n",
        "Looks preety cool. Let's try something else."
      ]
    },
    {
      "metadata": {
        "id": "nxdq-YGYAOzg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# K-Nearest Neighbors\n",
        "\n",
        "KNN algorithm is one of the simpler ones. It dosen't actually learn anything. It just memorizes all the points, and when you come up with new points, it calculates the closest points to it. The points then vote to get the class.\n",
        "\n",
        "**K refers to the number of neighbors**\n",
        "\n",
        "For example, in 3-NN, if a point is closer to 2 points belonging to class A, and a point belonging to class B, then the predicted class will be A. Check out these pictures for more intuition.\n",
        "\n",
        "\n",
        "![](https://image.slidesharecdn.com/machine-learning-and-data-mining-13-nearest-neighbor-and-bayesian-classifiers-2990/95/machine-learning-and-data-mining-13-nearest-neighbor-and-bayesian-classifiers-12-728.jpg?cb=1176615304)\n",
        "\n",
        "\n",
        "KNN documentation:\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
        "\n",
        "Further Reading\n",
        "- https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/"
      ]
    },
    {
      "metadata": {
        "id": "QzeLJhkJ80r-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's get back at the blobs."
      ]
    },
    {
      "metadata": {
        "id": "AgwvJTS28yWO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X, y = datasets.make_blobs(1000, centers=2, random_state=0)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=100)\n",
        "plt.gcf().set_size_inches(21, 14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mrNwQi_F8LRo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# 1 neighbor. Maybe arbitrary choice?\n",
        "knn = KNeighborsClassifier(n_neighbors=1)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "knn.fit(x_train, y_train)\n",
        "\n",
        "print(\"Train accuracy\", knn.score(x_train, y_train))\n",
        "print(\"Test accuracy score:\", knn.score(x_test, y_test))\n",
        "\n",
        "plot_decision_boundary(knn, X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OLy3w6QW9Abl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://scontent-frx5-1.cdninstagram.com/vp/5109f2bc581a68a0cf02dc7915ebcb60/5C479018/t51.2885-15/e35/40696869_395054887980371_6165376665352514362_n.jpg?se=7&ig_cache_key=MTg2MzI2NjE0MDM3NDIyNjIxMA%3D%3D.2)\n",
        "\n",
        "\n",
        "We got ~ 94% accuracy for 1-NN. But what about the choice of K? Your turn"
      ]
    },
    {
      "metadata": {
        "id": "nm30Cwx29JM_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Coding Challenge\n",
        "\n",
        "Find the best k for KNN, based on the test score."
      ]
    },
    {
      "metadata": {
        "id": "Bv_-_wzN9O8h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO split your data into train / test\n",
        "\n",
        "\n",
        "# put the test scores in this list\n",
        "scores = []\n",
        "\n",
        "for k in range(1, 50):\n",
        "  # TODO train your model with the current k\n",
        "  \n",
        "  # TODO Check the test accuracy of your model\n",
        "  # score = ...\n",
        "  \n",
        "  # TODO add the score to the list\n",
        "  \n",
        "  scores.append(score)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v487fzDF9pSW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's plot the scores for each K, and see how we did."
      ]
    },
    {
      "metadata": {
        "id": "Mom-UORz9o6o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(range(1, len(scores) + 1), scores)\n",
        "plt.gca().set_xticks(np.arange(1, len(scores) + 1))\n",
        "plt.gca().set_yticks(np.arange(np.min(scores), np.max(scores) + 0.001, 0.001))\n",
        "plt.gcf().set_size_inches(21, 14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aGnP4fle-sMb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's plot the decision boundry."
      ]
    },
    {
      "metadata": {
        "id": "XfryRFCE-5uO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=19)\n",
        "knn.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "plot_decision_boundary(knn, X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0N_0ur8sOHrI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Coding Challenge\n",
        "\n",
        "Train KNN classifier on the moons dataset. Find your best K by iterating over a range of values and finding the K that maximizez the test error."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "kY0iKqoZZDzk",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO find the best K and plot the decision boundry on this dataset.\n",
        "X, y = datasets.make_moons(1000, noise=0.2, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hHnkMMuTASGr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Decision Trees and Random Forest\n",
        "\n",
        "A decision tree is just a glorified set of **if** statements. But the catch is that they are **learned** by observing data.\n",
        " \n",
        "![](https://s3-ap-southeast-1.amazonaws.com/he-public-data/Fig%201-18e1a01b.png)\n",
        "\n",
        "The problem when learning decision trees figuring out what attributes contain the **most** information. That is achieved by maximizing the information gain on each attribute. Check out the further reading for more information.\n",
        "\n",
        "\n",
        "**Random forest** is a method of combining multiple smaller decision trees for a more robust and accuracte predictions - it's a type of ensemble.\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/592/1*i0o8mjFfCn-uD79-F1Cqkw.png)\n",
        "\n",
        "Further Reading:\n",
        "- https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052\n",
        "- https://www.saedsayad.com/decision_tree.htm\n",
        "- https://towardsdatascience.com/the-random-forest-algorithm-d457d499ffcd\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
      ]
    },
    {
      "metadata": {
        "id": "gntx2xLN_U8D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Third party libraries needed for visualisations\n",
        "!pip install graphviz\n",
        "!apt install graphviz\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qlG_L5M9T8oP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's see how a decision tree looks like after we've trained it on the blobs."
      ]
    },
    {
      "metadata": {
        "id": "QNM5jsgS_fMb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import graphviz\n",
        "from sklearn.tree import export_graphviz\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dot_data = export_graphviz(model,\n",
        "                           out_file=None,\n",
        "                           filled=True,\n",
        "                           rounded=True)\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1_JTclr0DuIV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_decision_boundary(model, X, y)\n",
        "\n",
        "print(\"Training accuracy\", model.score(x_train, y_train))\n",
        "print(\"Test accuracy\", model.score(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TOxwTcrvAhcp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Coding Challenge\n",
        "\n",
        "Remember that RandomForest combines multiple smaller trees to form a better estimator. Try out some values for n_estimators. "
      ]
    },
    {
      "metadata": {
        "id": "wDdK5rGDU72i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "# TODO find the optimal number of estimators in a RandomForest such that it maximizes the test accuracy (dosen't overfit).\n",
        "\n",
        "# TODO plot the decision boundry\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# model = RandomForestClassifier(n_estimators=...)\n",
        "# model.fit(...)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cnERpSkVAlAs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Support Vector Machines\n",
        "\n",
        "Support vector machines are one of the most powerful machine learning models. Since its invention in the '80s, it outperformed almost all types of models, until it was surpassed by neural networks in the 2000s.\n",
        "\n",
        "It works by finding the largest gap between the our classes, and putting the decision boundry (which is a line) such that it is as far as possible from the points. This way, it is very resistant to overfitting.\n",
        "\n",
        "\n",
        "\n",
        "![](https://66.media.tumblr.com/0e459c9df3dc85c301ae41db5e058cb8/tumblr_inline_n9xq5hiRsC1rmpjcz.jpg)\n",
        " \n",
        " However, the data **must** be linearly separable, or else it dosen't work properly. Let's look at the circles dataset.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "fkP28IB3FQ1g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X, y = datasets.make_circles(1000, noise=0.1, factor=0.5)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=100)\n",
        "plt.gcf().set_size_inches(21, 14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nVZbsUz0Ffqx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What do?\n",
        "\n",
        "Introducing the **kernel**. Transform the space into something else, that turns out to be linearly separable (by a plane).\n",
        "\n",
        "In fact, when we applied polynomial features to our data, we were applying a sort of kernel so we can more easly separate the classes. Working in a higher dimmensional space is sometimes easier. Have a look at this pretty image\n",
        "\n",
        "![](https://i.stack.imgur.com/7yM2K.png)\n",
        "\n",
        "We added another dimmension by applying a kernel. In this new space, the data is linearly separable by a plane, and SVM can work really good. Let's see it in action.\n",
        "\n",
        "Further Reading:\n",
        "- http://diggdata.in/post/94066544971/support-vector-machine-without-tears\n",
        "- https://en.wikipedia.org/wiki/Radial_basis_function_kernel"
      ]
    },
    {
      "metadata": {
        "id": "mLkIK0QCFJCU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "model = SVC(kernel='rbf', probability=True)\n",
        "model.fit(X, y)\n",
        "\n",
        "plot_decision_boundary(model, X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "son_SwvSA_Rd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Coding challenge\n",
        "\n",
        "Fit a SVM with and 'rbf' kernel and one 'poly' kernel on the moons dataset."
      ]
    },
    {
      "metadata": {
        "id": "vO370VwVX2Ps",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X, y = datasets.make_moons(1000, noise=0.2, random_state=0)\n",
        "\n",
        "# Careful to specifiy probability=True to the SVM, or else the decision boundry plot will fail.\n",
        "\n",
        "# TODO train a SVM with 'rbf' kernel\n",
        "\n",
        "# TODO plot the decision boundry\n",
        "\n",
        "# TODO train a SVM with 'poly' kernel\n",
        "\n",
        "# TODO plot the decision boundry"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RBK9TzHgor-P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "0tuS9dBw_3Xz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Imbalanced Learning"
      ]
    },
    {
      "metadata": {
        "id": "VQhdlkkU_3sY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# This is an imbalanced dataset. Be wary of accuracy!\n",
        "X, y = datasets.make_classification(n_samples=20000, \n",
        "                           n_features=2, \n",
        "                           n_informative=2,\n",
        "                           n_classes=2, \n",
        "                           n_clusters_per_class=1,\n",
        "                           n_redundant=0,\n",
        "                           weights=[0.90, 0.10],\n",
        "                           class_sep=0.0,\n",
        "                           random_state=2)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=100)\n",
        "plt.gcf().set_size_inches(21, 14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dn59OaKYACLu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.bar(np.arange(2), np.unique(y, return_counts=True)[1])\n",
        "plt.xticks(np.arange(2), ('0', '1'))\n",
        "plt.xlabel('Y')\n",
        "plt.ylabel('Y Counts')\n",
        "plt.gcf().set_size_inches(21, 14)\n",
        "plt.title('Class imbalance')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1mF43eaYANrr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Coding Challenge\n",
        "\n",
        "Train LogisticRegression on this dataset. Check the train or test accuracy. "
      ]
    },
    {
      "metadata": {
        "id": "-Mbv-hQDANPo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO do it."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8dyd8Pi1AX94",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Impressive, but it the model actually hasn't learned anything."
      ]
    },
    {
      "metadata": {
        "id": "yqef-BrbBaKJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Other Classification Metrics\n",
        "\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1600/1*CPnO_bcdbE8FXTejQiV2dg.png)\n",
        "\n",
        "Precision measures how many selected items are relevant.\n",
        "\n",
        "$Precision=\\frac{TP}{TP + FP}$\n",
        "\n",
        "Recall measures how many relevant items are selected.\n",
        "\n",
        "$Recall=\\frac{TP}{TP + FN}$\n",
        "\n",
        "F1 score is just the harmonic mean between the two, so that we can a have a single measure of our performance.\n",
        "\n",
        "\n",
        "$F_1=\\frac{2 \\cdot Precision \\cdot Recall}{Precision + Recall}$\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/350px-Precisionrecall.svg.png)\n",
        "\n",
        "Further Reading:\n",
        "- https://en.wikipedia.org/wiki/Precision_and_recall"
      ]
    },
    {
      "metadata": {
        "id": "EOgFodXzB4q4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Coding Challenge\n",
        "\n",
        "Check the confusion matrix of your trained model and the precision and recall."
      ]
    },
    {
      "metadata": {
        "id": "xlwuLojPBzx8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# TODO call classification report\n",
        "\n",
        "# classification_report(model.predict(x_test), y_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wx2BdaXpCZkT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install imbalanced-learn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2bkX5A7-CViE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Over-Sampling and Down-Sampling\n",
        "\n",
        "To deal with imbalanced datasets, one strategy is to over-sample the minority class, or to downsample the majority class. For now, we will use \n",
        "\n",
        "[**Synthetic minority over-sampling technique**](https://arxiv.org/pdf/1106.1813.pdf)\n",
        "\n",
        "We'll use [imbalanced-learn](https://imbalanced-learn.readthedocs.io/en/stable/api.html) python package.\n",
        "\n",
        "Check out this article on how to properly apply SMOTE:\n",
        "\n",
        "[The Right Way to Oversample in Predictive Modeling](https://beckernick.github.io/oversampling-modeling/)\n",
        "\n",
        "The gist of it is that you should first split your data into train and test and **then** apply SMOTE. Otherwise, if you first over-sample and then split, some of the oversampled data will be present in the test set and then the evaluation will be incorrect!"
      ]
    },
    {
      "metadata": {
        "id": "d7NWHjlTDP1e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Coding Challenge for True Heroes\n",
        "\n",
        "Use \"Synthetic minority over-sampling\" to over sample your data and train your model."
      ]
    },
    {
      "metadata": {
        "id": "aNzinFjpCeaK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, confusion_matrix, \n",
        "import imblearn.over_sampling as over_sampling\n",
        "\n",
        "smote = over_sampling.SMOTE(ratio=1.0)\n",
        "\n",
        "# TODO 1: Split your data into train and test (using train_test_split).\n",
        "\n",
        "# TODO 2: Use SMOTE to oversample the *training data*.\n",
        "\n",
        "# i.e. smote.fit_sample(x_train, y_train)\n",
        "\n",
        "# TODO 3: Train a model on the oversampled data.\n",
        "\n",
        "# TODO 4: Evaluate the model on the test set with the proper metrics.\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K2JZP89zZOp6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Thanks for your attention!"
      ]
    }
  ]
}