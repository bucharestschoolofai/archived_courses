{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Building a Neural Network Crash Course.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "RW1h83P9I2DM"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "VRxKNwDfCGKK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Presentation\n",
        "Please find the slideshow presentation [here](https://docs.google.com/presentation/d/14uD1zIE6CEnc8c_PvxOxPLazJZEj5kHqmsB6oeqbqyY/edit?usp=sharing)"
      ]
    },
    {
      "metadata": {
        "id": "RW1h83P9I2DM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports and utility functions"
      ]
    },
    {
      "metadata": {
        "id": "p6ulgrB5I9Q-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import types\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "plt.rcParams['image.cmap'] = 'RdBu'\n",
        "\n",
        "import sklearn.datasets as datasets\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "def plot_decision_boundary(model, X, y, degree=1):\n",
        "    \"\"\"\n",
        "    Use this to plot the decision boundary of a trained model.\n",
        "    \"\"\"\n",
        "    grid_lim = np.array([[X[:,0].min(), X[:,0].max()], [X[:,1].min(), X[:,1].max()]])\n",
        "    xx, yy = np.mgrid[grid_lim[0,0]:grid_lim[0,1]:.01, \n",
        "                      grid_lim[1,0]:grid_lim[1,1]:.01]\n",
        "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "    \n",
        "    t = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    _poly = t.fit_transform(grid)\n",
        "    \n",
        "    probs = model.predict_proba(_poly)[:, 1].reshape(xx.shape)\n",
        "    \n",
        "    f, ax = plt.subplots(figsize=(8, 6))\n",
        "    contour = ax.contourf(xx, yy, probs, 25, cmap=\"RdBu\",\n",
        "                        vmin=0, vmax=1)\n",
        "    ax_c = f.colorbar(contour)\n",
        "    ax_c.set_label(\"$P(y = 1)$\")\n",
        "    ax_c.set_ticks([0, .25, .5, .75, 1])\n",
        "\n",
        "    ax.scatter(X[:,0], X[:, 1], c=y, s=100,\n",
        "             cmap=\"RdBu\", vmin=-.2, vmax=1.2,\n",
        "             edgecolor=\"white\", linewidth=1)\n",
        "\n",
        "    ax.set(aspect=\"equal\",\n",
        "           xlim=(grid_lim[0,0],grid_lim[0,1]), \n",
        "           ylim=(grid_lim[1,0],grid_lim[1,1]),\n",
        "           xlabel=\"$X_1$\", ylabel=\"$X_2$\")\n",
        "    plt.gcf().set_size_inches(21, 14)\n",
        "    return f, ax\n",
        "  \n",
        "  \n",
        "\n",
        "def plot_history(h):\n",
        "  \n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "  \n",
        "  ax1.plot(h.history['loss'], label='Training Loss')\n",
        "  ax1.plot(h.history['val_loss'], label='Test Loss')\n",
        "  ax1.set_ylabel('Loss')\n",
        "  ax1.set_xlabel('Epoch')\n",
        "  ax1.legend(fontsize=24)\n",
        "  ax1.set_ylim(0, 1)\n",
        "  \n",
        "  ax2.plot(h.history['acc'], label='Training Accuracy')\n",
        "  ax2.plot(h.history['val_acc'], label='Test Accuracy')\n",
        "  ax2.set_ylabel('Accuracy')\n",
        "  ax2.set_xlabel('Epoch')\n",
        "  ax2.legend(fontsize=24)\n",
        "  ax2.set_ylim(0, 1)\n",
        "  \n",
        "  fig.suptitle('Evolution over epochs', fontsize=24)\n",
        "  \n",
        "  fig.set_size_inches(21, 14)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ep1z9fy-FeiB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Building a Neural Network Crash Course"
      ]
    },
    {
      "metadata": {
        "id": "eKH3Jqij-mWv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Important things to remember\n",
        "\n",
        "1. Neural networks are not magic, just math\n",
        "2. There are no recipes for building neural networks, only \"best practices\"\n",
        "3. Neural networks can learn anything, given enough data"
      ]
    },
    {
      "metadata": {
        "id": "ekDrkb1KYL1S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Regression"
      ]
    },
    {
      "metadata": {
        "id": "rslWR6WgAeEY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First things first, let's import **keras**. Keras is a python library that makes it easier to use Google's Tensorflow, the most popular library for building neural networks. "
      ]
    },
    {
      "metadata": {
        "id": "3Qtc9evG_HQQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PtVQqTaFYoXx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's use the Boston Housing dataset we used in Epoch 2."
      ]
    },
    {
      "metadata": {
        "id": "E6jjsDsEX_6T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "file = urlopen('https://raw.githubusercontent.com/bucharestschoolofai/epoch_2/master/train.csv')\n",
        "data = pd.read_csv(file, delimiter=',', usecols=['SalePrice', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'TotalBsmtSF', 'FullBath', 'GarageCars', 'Fireplaces'])\n",
        "\n",
        "X = data.loc[:, :'GarageCars'].values\n",
        "y = data.loc[:, 'SalePrice':].values\n",
        "\n",
        "data.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BLdNov-vdvae",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's define our model."
      ]
    },
    {
      "metadata": {
        "id": "yVLsBX21Ys3D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "\n",
        "# First layer needs the input shape to be specified. We're dealing with 8D points, so input_shape=(8,)\n",
        "model.add(keras.layers.Dense(units=4, input_shape=(8,), activation='relu', name='layer_1'))\n",
        "\n",
        "# ReLU for the activation function\n",
        "model.add(keras.layers.Dense(units=4, activation='relu',  name='layer_2'))\n",
        "\n",
        "# The final layer has 1 neuron because we predict one value, the price\n",
        "model.add(keras.layers.Dense(units=1, activation='relu',  name='layer_3'))\n",
        "\n",
        "# Loss is Mean Absolute Error. \n",
        "model.compile(loss='mean_absolute_error', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PfTT-zCQYwit",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-hr3HuBWY3ob",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(X, y, batch_size=1, epochs=10, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QdvaNepabK8U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Coding Challenge: Make the neural network more powerful\n",
        "- increase the number of neurons\n",
        "- increase the number of layers\n",
        "- change activation functions"
      ]
    },
    {
      "metadata": {
        "id": "pguSrFF5a68g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Classification"
      ]
    },
    {
      "metadata": {
        "id": "SBokbenf_IiF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's get back to the moons dataset that we tackled in Epoch 3 using classical machine learning"
      ]
    },
    {
      "metadata": {
        "id": "FyH39Qi9_S1I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X, y = datasets.make_moons(3000, noise=0.2, random_state=0)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=100)\n",
        "plt.gcf().set_size_inches(21, 14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JmTbKwD-_5eG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll try to use this dataset to learn about building neural networks. Later on we'll see how ca we teach neural model to \"**read**\" some numbers."
      ]
    },
    {
      "metadata": {
        "id": "APRDeD4gA_49",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's build a neural network with 3 layers that can classify these points."
      ]
    },
    {
      "metadata": {
        "id": "2KnIS92DA-pR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "\n",
        "\n",
        "# First layer needs the input shape to be specified. We're dealing with 2D points, so input_shape=(2,)\n",
        "model.add(keras.layers.Dense(units=4, input_shape=(2,), activation='relu', name='layer_1'))\n",
        "\n",
        "# ReLU for the activation function\n",
        "model.add(keras.layers.Dense(units=4, activation='relu',  name='layer_2'))\n",
        "\n",
        "# Softmax activation to output probabilities \n",
        "model.add(keras.layers.Dense(units=2, activation='softmax',  name='layer_3'))\n",
        "\n",
        "# Loss is crossentropy, like in LogisticRegression! Optimizer is Stochastic Gradient Descent and we are interested in accuracy.\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AhXgmGW6A-v5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gASfvqheBzf2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What's going on here? We've just made our model. But what does that mean? We actually create 3 matrices. Let's print them out:"
      ]
    },
    {
      "metadata": {
        "id": "QvjTrvesA-08",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for layer in model.layers:\n",
        "  weights, bias = layer.get_weights()\n",
        "  \n",
        "  print(f\"### Layer \", layer.name, \"Neurons: \", layer.input_shape[1])\n",
        "  print(\"Weights: \\n\", weights)\n",
        "  print(\"Bias: \", bias)\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BGKyCxM1DGps",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The shapes of these matrices are such that they connect the current layer to the next layer (i.e. the first matrix is 2x4 because the first layer has 2 neurons and the next one has 4 neurons).\n",
        "\n",
        "Let's get to training!"
      ]
    },
    {
      "metadata": {
        "id": "tjuAx2pBA-3j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(X, y, batch_size=1, epochs=5, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ME7aNiLPA-5_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_decision_boundary(model, X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vbwiV8wQFNKI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Coding Challenge: Make the neural network more powerful\n",
        "- increase the number of neurons\n",
        "- increase the number of layers\n",
        "- change activation functions\n"
      ]
    },
    {
      "metadata": {
        "id": "XYSFQ_pxIlSA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Initializers"
      ]
    },
    {
      "metadata": {
        "id": "S6BjMlwZMmI3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We saw that the matrix weghits were some random numbers. In keras, we can specify the initialization function in the arguments. Let's initialize the weights with zeros and see what happens."
      ]
    },
    {
      "metadata": {
        "id": "xyNwawk4MNhb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "\n",
        "model.add(keras.layers.Dense(units=4, input_shape=(2,), activation='relu', kernel_initializer='zeros', bias_initializer='zeros'))\n",
        "\n",
        "model.add(keras.layers.Dense(units=4, activation='relu'))\n",
        "\n",
        "model.add(keras.layers.Dense(units=2, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X, y, batch_size=1, epochs=5, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XE-xQghfIo9_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Learning Rate"
      ]
    },
    {
      "metadata": {
        "id": "d6k2SK6OIsAF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The learning rate hyperparameter can be very important, as a too low learning rate can make the model get stuck in a local minima, and a higher learning rate can make the model overshoot. \n",
        "\n",
        "A good rule of thumb is to start with a higher learning rate initially, and then decrease it as the learning progresses."
      ]
    },
    {
      "metadata": {
        "id": "Fn3lNCU8OA9o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "\n",
        "model.add(keras.layers.Dense(units=4, input_shape=(2,), activation='relu'))\n",
        "\n",
        "model.add(keras.layers.Dense(units=4, activation='relu'))\n",
        "\n",
        "model.add(keras.layers.Dense(units=2, activation='softmax'))\n",
        "\n",
        "# TODO play with the learning rate hyperparameter\n",
        "sgd = keras.optimizers.SGD(lr=0.01)\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X, y, batch_size=1, epochs=5, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gg-FG9n4Az3z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Reading Digits (Classification Extended)"
      ]
    },
    {
      "metadata": {
        "id": "_HpXOsYwFiZb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's teach a neural network to **READ** some digits. We will be using the classic [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database).\n"
      ]
    },
    {
      "metadata": {
        "id": "C5vk1tvNFDQr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EjQ6wEfPFbPo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
        "\n",
        "ax1.imshow(x_train[0], cmap='gray')\n",
        "ax1.set_title('Label:' + str(y_train[0]))\n",
        "\n",
        "\n",
        "ax2.imshow(x_train[1], cmap='gray')\n",
        "ax2.set_title('Label:' + str(y_train[1]))\n",
        "\n",
        "ax3.imshow(x_train[2], cmap='gray')\n",
        "ax3.set_title('Label:' + str(y_train[2]))\n",
        "\n",
        "fig.set_size_inches(21, 14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QLomvxU5PiwW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To process these images with some fully connected neural network, we need to \"flatten\" them, because these types of layers can handle only vectors as input."
      ]
    },
    {
      "metadata": {
        "id": "7wMdU1E2HfTM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "\n",
        "\n",
        "model.add(keras.layers.Flatten(input_shape=(x_train.shape[1], x_train.shape[2])))\n",
        "model.add(keras.layers.Dense(10, activation='relu'))\n",
        "model.add(keras.layers.Dense(10, activation='relu'))\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S3LHRBFHO4Q5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://www.superdatascience.com/wp-content/uploads/2018/08/CNN_Step3_Img1.png)"
      ]
    },
    {
      "metadata": {
        "id": "I-83TFDOAjZA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Flattening inherently neglects valuable spatial information, therefore using fully-connected layers with flattening is not the best approach for analysing images."
      ]
    },
    {
      "metadata": {
        "id": "7qKU9Ea_Idju",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YlDVux5_LlSy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(model, show_layer_names=True, show_shapes=True, to_file='model.png')\n",
        "\n",
        "from IPython.display import Image\n",
        "\n",
        "Image(filename='model.png', height=600)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GKGrZ9KdHrJP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "h = model.fit(x_train / 255., y_train, # our data\n",
        "              epochs=10, # number of passes\n",
        "              batch_size=32, # number of images per training step\n",
        "              validation_data=(x_test / 255., y_test)) # validation data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x7sqrxrJITx1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_history(h)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xos8B7MWcoOo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Coding Challenge: Make the neural network more powerful\n",
        "- increase the number of neurons\n",
        "- increase the number of layers\n",
        "- change activation functions\n"
      ]
    },
    {
      "metadata": {
        "id": "dtCrYugaSNmQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# But how to decide on how many layers and neurons?\n",
        "\n",
        "No clear answer here. You'll have to decide the optimal architecture for your problem and data. But you can also take this formula as a guide:"
      ]
    },
    {
      "metadata": {
        "id": "0GLULTyRUw6s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$\\begin{equation}\n",
        "N_h = \\dfrac{N_s}{\\alpha * (N_i + N_o)}\n",
        "\\end{equation}$\n",
        "\n",
        "$N_h$ = number of neurons in the hidden layers\n",
        "\n",
        "$N_i$ = number of input neurons\n",
        "\n",
        "$N_o$ = number of output neurons\n",
        "\n",
        "$N_s$ = number of samples\n",
        "\n",
        "$\\alpha$ = arbitrary scaling factor, usually 2-10 \n"
      ]
    },
    {
      "metadata": {
        "id": "acfGmQzNShyo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**NOTE**\n",
        "This only applies to **fully-conected** layers. The **UNIVERSAL APROXIMATION THEOREM** states that a fully-conected neural network with only one hidden layer can learn any function (with arbitrary number of neurons, of course)"
      ]
    },
    {
      "metadata": {
        "id": "r8uiVNoFS5aS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# But can we do better? (Teaser)"
      ]
    },
    {
      "metadata": {
        "id": "PmPfCZ0KTPCx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Yes we can. In fact, fully-conected neural networks are not appropiate for working with images. There is another type of neural network that is purposely built for this type of data. That is the **Convolutional Neural Network**, or CNN, for short, which we will cover in detail **in the upcoming event**."
      ]
    },
    {
      "metadata": {
        "id": "QEAEGISbLVYr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu',input_shape=(28, 28, 1)))\n",
        "\n",
        "model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(keras.layers.Dropout(0.25))\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(128, activation='relu'))\n",
        "model.add(keras.layers.Dropout(0.5))\n",
        "\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(np.expand_dims(x_train, -1) / 255, y_train, batch_size=32, epochs=3, validation_data=(np.expand_dims(x_test, -1) / 255, y_test))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}