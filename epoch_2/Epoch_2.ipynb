{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Epoch 2",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "oQ5K-SvABEA3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "%matplotlib inline\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iXOTbrC-apLz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Epoch 2: Relations in Data\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "SwOHVlG7kZXW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Table of Contents\n",
        "\n",
        "- Why regression? \n",
        "- What is regression\n",
        "- Types of correlation\n",
        "- A naive approach\n",
        "- Evaluating our approach\n",
        "- Getting to the optimal regression line\n",
        "- Improving our technique\n",
        "- Working with higher degrees\n",
        "- Underfitting, Overfitting, Train/Test Split\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "gAXLaWUfo7Gq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Why linear regression?\n",
        "\n",
        "It's one of the most basic methods for modeling linear data. It's just a line! Almost every other model in machine learning uses as its core some form of linear regression (or properties of linearity) - even the most complex neural networks. \n",
        "\n",
        "While it is a simple model, it has a lot of generalizing power - you can extend the line indefinitely to handle points we've never seen before."
      ]
    },
    {
      "metadata": {
        "id": "lhjFiFeo_vdS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://i1.wp.com/erickimphotography.com/blog/wp-content/uploads/2013/10/diagonal.jpg?resize=619%2C415)\n",
        "\n",
        "A \"linear model\"."
      ]
    },
    {
      "metadata": {
        "id": "hJZSle7kkc9q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## What is Regression?\n",
        "\n",
        "The basic idea behind linear regression is to find the **best** line between a set of points. Let's take some data on houses to try to fit a line between them. The goal here is to try to predict the price of a house, for houses we've never seen before.\n"
      ]
    },
    {
      "metadata": {
        "id": "HA3ECXWCBCe9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seismic_risk = np.array([9.3, 8.5, 7.2, 6.8, 5.3, 4.2, 3.7, 2.1, 1.4, 0.7]).reshape(-1, 1)\n",
        "month_sold = np.array([4, 6, 12, 8, 2, 11, 4, 8, 6, 7]).reshape(-1, 1)\n",
        "year_sold = np.array([89, 91, 91, 93, 95, 98, 99, 99.5, 100, 102]).reshape(-1, 1)\n",
        "\n",
        "sale_price = np.array([45, 52, 60, 68, 77, 82, 90, 93, 102, 107])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "szaZuyhKCB8S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In machine learning, we call *seismic_risk, month_sold* and *year_sold* **features**, or independent variables, while *sale_price* is called the **label**, or dependent variable.\n",
        "\n",
        "Features are usually marked with an ***X***, and labels with a ***y***. Let's do this now:"
      ]
    },
    {
      "metadata": {
        "id": "10wQw7NgDP8c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = np.hstack([seismic_risk, month_sold, year_sold])\n",
        "X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-eGDZsUwED4u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y = sale_price.reshape(-1, 1)\n",
        "y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oPhZ4RgoGkci",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And so, ***Linear Regression*** tries to find $w$ and $w'$ such that the line $y' = w\\textbf{X} + w'$ is the best fit for the data provided. $\\textbf{w}$ is a vector called the *coefficient vector*  or *slope*, and $w'$ is a vector called *the intercept*. "
      ]
    },
    {
      "metadata": {
        "id": "1peo6JfqEg6I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ok, now we are ready to do some analysis on the House Prices Mini-Dataset."
      ]
    },
    {
      "metadata": {
        "id": "8PzsUhQApAWS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# A Naive Approach"
      ]
    },
    {
      "metadata": {
        "id": "8W31yo1LF0KC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ok, for starters, let's just try to predict the house price using only the *year_sold* feature."
      ]
    },
    {
      "metadata": {
        "id": "Rq_6RefYGAT1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(year_sold, sale_price, 'o')\n",
        "plt.xlabel(\"Year Sold\")\n",
        "plt.ylabel(\"Sale Price ($ x1000)\")\n",
        "plt.title(\"House prices depending on the year they were sold.\")\n",
        "plt.gcf().set_size_inches(14, 7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Cax97xxsXAN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## An Intuitive Linear Function\n",
        "\n",
        "The data is looks linear enough. How should we draw this line?"
      ]
    },
    {
      "metadata": {
        "id": "ykQmU_bQJQ9H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_line(w, w_, start, end):\n",
        "  X = np.linspace(start, end, 1000).reshape(-1, 1)\n",
        "  y = w * X + w_\n",
        "  \n",
        "  plt.plot(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p-Iq6yMpIxdA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# My guesstimate\n",
        "w = 3\n",
        "w_ = -220\n",
        "\n",
        "\n",
        "plt.plot(year_sold, sale_price, 'o')\n",
        "\n",
        "plot_line(w, w_, year_sold.min(), year_sold.max())\n",
        "\n",
        "plt.xlabel(\"Year Sold\")\n",
        "plt.ylabel(\"Sale Price ($ x1000)\")\n",
        "plt.title(\"House prices depending on the year they were sold.\")\n",
        "plt.gcf().set_size_inches(14, 7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NcKeaQCFL2Qj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It definitely looks like a line! Though something is a bit off. Why is this not the best line? How do you know that?"
      ]
    },
    {
      "metadata": {
        "id": "cAYSZX-7pjEh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Coding Challenge\n",
        "\n",
        "Try this yourself! Modify **w** and **w_** so the line better fits the points."
      ]
    },
    {
      "metadata": {
        "id": "lc8f7ARFpk6p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO: modify w and w_ to get a better line. Rerun this cell as many times as you have to.\n",
        "w = 3\n",
        "w_ = -220\n",
        "\n",
        "\n",
        "plt.plot(year_sold, sale_price, 'o')\n",
        "\n",
        "plot_line(w, w_, year_sold.min(), year_sold.max())\n",
        "\n",
        "plt.xlabel(\"Year Sold\")\n",
        "plt.ylabel(\"Sale Price ($ x1000)\")\n",
        "plt.title(\"House prices depending on the year they were sold.\")\n",
        "plt.gcf().set_size_inches(14, 7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l3i6wwfSsmQV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Our Predictions\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "gRgAPX58N4Hq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's use the regression line we made to estimate the house prices for the houses we know about."
      ]
    },
    {
      "metadata": {
        "id": "bZqV5SFsNiC_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_ = w * year_sold + w_\n",
        "\n",
        "\n",
        "plt.plot(year_sold, sale_price, 'o')\n",
        "plt.plot(year_sold, y_, 'o')\n",
        "\n",
        "plt.xlabel(\"Year Sold\")\n",
        "plt.ylabel(\"Sale Price ($ x1000)\")\n",
        "plt.title(\"House prices depending on the year they were sold.\")\n",
        "plt.gcf().set_size_inches(14, 7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gWhX_BfxgUVg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Looks almost right. We probabily could do better.\n",
        "\n",
        "\n",
        "![](https://i.stack.imgur.com/u5HhK.png)"
      ]
    },
    {
      "metadata": {
        "id": "zNX0YIHjpJke",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Evaluating our Approach"
      ]
    },
    {
      "metadata": {
        "id": "PqeiHTr2sQFe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Residuals\n",
        "\n",
        "The difference between the observed value of the dependent variable (y) and the predicted value (y') is called the [residual](https://en.wikipedia.org/wiki/Errors_and_residuals) (e). We can use the residual plot to further analyze our model, and to understand it better. "
      ]
    },
    {
      "metadata": {
        "id": "n5EA1kfpOZkN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "errors = y - y_\n",
        "\n",
        "plt.plot(year_sold, errors, 'o', c='red')\n",
        "plt.axhline(0, c='black')\n",
        "\n",
        "for e, yr in zip(errors, year_sold):\n",
        "  plt.plot([yr, yr], [0, e], '--', c='gray')\n",
        "\n",
        "plt.xlabel(\"Year Sold\")\n",
        "plt.ylabel(\"Sale Price Error\")\n",
        "plt.title(\"Residual plot\")\n",
        "\n",
        "plt.gcf().set_size_inches(14, 7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RaykIfz1sSef",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Error\n",
        "\n",
        "We calculated the error simply as:\n",
        "\n",
        "$$e_i = (y_i - y_i')$$\n",
        "\n",
        "We could consider a better formula for the error, that actually gives us a number for the whole dataset:\n",
        "\n",
        "$$ e = \\frac{\\sum_i(y_i - y_i')^2}{n}$$\n",
        "\n",
        "This is called the *Mean Squared Error*. The reason it is squared is that we would want to further penalize big errors. We will see that this is actually the loss function for the Linear Regression model. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "8SwZEhMEo3jH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Types of Correlation"
      ]
    },
    {
      "metadata": {
        "id": "rVtWeKdTEelO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We've seen that the lines you chose generally have the same **slope** for *year_sold*. What does that tell us?"
      ]
    },
    {
      "metadata": {
        "id": "tBA7gxTWalza",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
        "\n",
        "sns.regplot(year_sold, sale_price, ax=ax1)\n",
        "ax1.set_title(\"Positive correlation\")\n",
        "ax1.set_xlabel(\"Year Sold ( + 1900)\")\n",
        "ax1.set_ylabel(\"Sale Price\")\n",
        "\n",
        "\n",
        "sns.regplot(month_sold, sale_price, ax=ax2)\n",
        "ax2.set_title(\"No correlation\")\n",
        "ax2.set_xlabel(\"Month Sold\")\n",
        "ax2.set_ylabel(\"Sale Price\")\n",
        "\n",
        "sns.regplot(seismic_risk, sale_price, ax=ax3)\n",
        "ax3.set_title(\"Negative correlation\")\n",
        "ax3.set_xlabel(\"Seismic Risk\")\n",
        "ax3.set_ylabel(\"Sale Price\")\n",
        "ax3.set_xlim(0, 10)\n",
        "ax3.set_ylim(40, 120)\n",
        "\n",
        "plt.gcf().set_size_inches(21, 14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I08bZ_K7fmJ0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### What is correlation, though?"
      ]
    },
    {
      "metadata": {
        "id": "0ZPDwan_fDvx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We see that *positive* slope indicates a positive correlation, *negative* slope indicates a negative correlation, and a slope close to 0 indicates no correlation.\n",
        "\n",
        "Positive correlation means that if we increase the value of one variable, the other variable increases as well. Similarly, negative correlation means that if we increase the value of one variable, the other variable *decreases* in value.\n",
        "\n",
        "\n",
        "There's actually a number between -1 and 1 that tells us just how correlated two variables are. That number is called [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)"
      ]
    },
    {
      "metadata": {
        "id": "2f2f1-XsNTX2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![](http://imgs.xkcd.com/comics/linear_regression.png)\n"
      ]
    },
    {
      "metadata": {
        "id": "cIXjSeZr3lYe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Coding Challenge\n",
        "\n",
        "Calculate the pearsonr between every feature and the targets (SalePrice). Check out the [documentation](https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.pearsonr.html) for Pearson's r."
      ]
    },
    {
      "metadata": {
        "id": "sny_90tE3emE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "# TODO apply pearsonr the features to see how correlated are they with the target."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X1M4PT1GpNSm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Getting to the Optimal Regression Line"
      ]
    },
    {
      "metadata": {
        "id": "oZD3wCn4szgB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The Algorithm\n",
        "\n",
        "Finding the optimal regression line means optimizing the loss function between the predicted values $y'$ and the actual, ground-truth labels, $y$:\n",
        "\n",
        "$$ loss = \\frac{\\sum_i(y_i' - y_i) ^ 2}{n}$$\n",
        "\n",
        "Linear Regression has a *closed form*, which means we don't need to do *gradient descent* (which neural networks will require) - we can calculate the coefficients directly by using these formulas:\n",
        "\n",
        "$$ w_1 = \\frac{\\sum_i[(x_i-\\bar{x})(y_i-\\bar{y})]}{\\sum_i(x_i-\\bar{x})^2} $$\n",
        "\n",
        "$$ w_0 = \\displaystyle{\\frac{1}{n} (\\sum_i y_i - w_1 \\sum_i x_i)} = \\bar{y} - w_1 \\bar{x}  $$\n",
        "\n",
        "\n",
        "Here, $\\bar{x}$ and $\\bar{y}$ means the average value for $x$ and $y$, respectively. You don't need to worry about this, *sklearn* does this automatically for us! \n",
        "\n",
        "### Further reading\n",
        "- https://machinelearningmastery.com/linear-regression-for-machine-learning/"
      ]
    },
    {
      "metadata": {
        "id": "7O6G-fNys7ET",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implementation\n",
        "Let's use sklearn's [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) class to find the best line for our points!"
      ]
    },
    {
      "metadata": {
        "id": "gbGg0c_Sesj4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(year_sold, y)\n",
        "\n",
        "w = model.coef_[0][0]\n",
        "w_ = model.intercept_[0]\n",
        "\n",
        "\n",
        "y_ = w * year_sold + w_\n",
        "\n",
        "plt.plot(year_sold, sale_price, 'o')\n",
        "\n",
        "plot_line(w, w_, year_sold.min(), year_sold.max())\n",
        "\n",
        "plt.xlabel(\"Year Sold\")\n",
        "plt.ylabel(\"Sale Price ($ x1000)\")\n",
        "plt.title(\"House prices depending on the year they were sold.\")\n",
        "plt.gcf().set_size_inches(14, 7)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6BRuZFp4iKNv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now this looks much better! You may have been close. But not ***exactly*** perfect fit, because you didn't optimize the loss function.\n",
        "\n",
        "How do we know if our data is really linear? That is, how do we know a linear model actually works on our data? sklearn's LinearRegression implements a score() function to give us a number to work with. It's actually [Coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination), which tells us what proportion of the variance in the dependent variable that is predictable from the independent variable(s)."
      ]
    },
    {
      "metadata": {
        "id": "Xv4TEliSj4cg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.score(year_sold, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "50raSIELkImW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is pretty good! 96% of the [variance](https://en.wikipedia.org/wiki/Variance) in our data is explained by our model. Let's get more into the solution space for our problem."
      ]
    },
    {
      "metadata": {
        "id": "DVe9n2Ezs8fE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2D Solution Space\n"
      ]
    },
    {
      "metadata": {
        "id": "HqL1qJjxUMIQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model = LinearRegression()\n",
        "model.fit(year_sold, y)\n",
        "\n",
        "w1_ideal = model.coef_[0][0]\n",
        "w0_ideal = model.intercept_[0]\n",
        "\n",
        "\n",
        "size = 10\n",
        "heatmap = []\n",
        "\n",
        "w1s = np.linspace(w1_ideal / 2, 3 * w1_ideal / 2, size).reshape(-1, 1)\n",
        "\n",
        "for w1 in w1s:\n",
        "  w0s = np.linspace(w0_ideal / 2, 3 * w0_ideal / 2, size).reshape(-1, 1)\n",
        "  \n",
        "  for w0 in w0s:\n",
        "    y_ = w1 * year_sold + w0\n",
        "    error = np.average((y - y_) ** 2)\n",
        "    heatmap.append(error)\n",
        "  \n",
        "\n",
        "w = np.array(heatmap).reshape(size, size)\n",
        "sns.heatmap(w, annot=True)\n",
        "\n",
        "\n",
        "plt.gcf().set_size_inches(21, 14)\n",
        "ax = plt.gca()\n",
        "ax.set_xticks([], [])\n",
        "ax.set_yticks([], [])\n",
        "\n",
        "ax.set_xlabel('Slope')\n",
        "ax.set_ylabel('Intercept')\n",
        "\n",
        "ax.set_title(\"Coefficient search space\")\n",
        "\n",
        "print(\"Minimum error:\", np.average(w1_ideal * year_sold + w0_ideal - y)**2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6jly4QskUeD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We see that it's a strip of \"optimal\" values for our problem. But only one, at the center, is perfect. But what about the values close to this optimal center? We will se how Ridge and Lasso regularization takes advantage of this to create a more robust model by forcing the slope and intercept to have smaller values."
      ]
    },
    {
      "metadata": {
        "id": "TbT0JmJH_W47",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is actually a plot of the loss function. It looks like a second degree polynomial - because it is. Check out the **Further reading** section below.\n",
        "\n",
        "\n",
        "## Further reading\n",
        "\n",
        "- https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0"
      ]
    },
    {
      "metadata": {
        "id": "_jYAfsHDqB1M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Coding Challenge\n",
        "\n",
        "Try to use sklearn's LinearRegression class to predict the salary of an employee based on the level of seniority. Complete the TODO's below.\n"
      ]
    },
    {
      "metadata": {
        "id": "1UpD2UVaqt1z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seniority = np.linspace(0, 25, 100).reshape(-1, 1)\n",
        "salary =  (2.5 * np.linspace(20, 64, 100) ** 2 - 100 * np.linspace(20, 64, 100) + np.random.uniform(0, 500, 100) + 1000).reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6KlrMzZki5pu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(seniority, salary, 'o', markersize=5)\n",
        "\n",
        "plt.gcf().set_size_inches(14, 7)\n",
        "plt.xlabel(\"Seniority\")\n",
        "plt.ylabel(\"Salary\")\n",
        "plt.title(\"Employee Salary Evolution\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JM2I_TPTk0zs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO fit a sklearn LinearRegression on this dataset to predict someone's salary based on the level of seniority. \n",
        "\n",
        "\n",
        "# TODO Plot the regression line and the residuals. \n",
        "\n",
        "\n",
        "# TODO Check the coefficient of determination."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9q-DdcbbpSqF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Improving our technique\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "p2wnetcEmy2d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Looks like our data is not really that linear! Actually, is a second degree polynomial, so **linear** regression won't be that great. There is a way to compute a regression model on this dataset with some clever [feature engineering](https://en.wikipedia.org/wiki/Feature_engineering)."
      ]
    },
    {
      "metadata": {
        "id": "9cLgblMitkHN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dealing with underfitting"
      ]
    },
    {
      "metadata": {
        "id": "98-G16P0oUam",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our model [**underfitted**](https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76). Underfitting means that our model is not complex enough to capture the distribution of our data. How can we make our model more complex?"
      ]
    },
    {
      "metadata": {
        "id": "avFh8IkgtUN3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The Algorithm\n",
        "\n",
        "We can, in fact, create additional features for this. The easiest way to do this is to just multiply each point by itself. That way we can fit a **polynomial** to our data.\n",
        "\n",
        "### Note: this is still linear regression - the model is linear in the coefficients, not in the features.\n",
        "\n",
        "Our 1-d dataset:\n",
        "$X = \\begin{bmatrix}\n",
        "x_1  \\\\\n",
        "x_2  \\\\\n",
        "x_3 \\\\\n",
        "\\vdots  \\\\\n",
        "x_n \\\\\n",
        "\\end{bmatrix}$\n",
        "becomes, after applying this feature engineering method:\n",
        "\n",
        "\n",
        "$X_{poly} = \\begin{bmatrix}\n",
        "1 & x_1^1 & x_1^2 \\\\\n",
        "1 & x_2^1 & x_2^2 \\\\\n",
        "1 & x_3^1 & x_3^2 \\\\\n",
        "\\vdots & \\vdots & \\vdots \\\\\n",
        "1 & x_n^1 & x_n^2\\\\\n",
        "\\end{bmatrix}$\n",
        "\n",
        "\n",
        "Now, we are trying to fit $X_{poly}$ to our targets $y$. Now we'll have 3 weights, one for each degree (previously, we considered the intercept the weight to the 0th degree):\n",
        "\n",
        "$W = \\begin{bmatrix}\n",
        "w_1  \\\\\n",
        "w_2  \\\\\n",
        "w_3 \\\\\n",
        "\\end{bmatrix}$\n",
        "\n",
        "As such, we have to find $\\textbf{W}$, such that the error between the predicted values and the targets is minimal.\n",
        "\n",
        "$\\textbf{X}_{poly}\\textbf{W} = \\textbf{Y'}$\n",
        "\n",
        "This is matrix multiplication. The formulas for $\\textbf{W}$ are a bit more complicated this time, but fear not! Sklearn has got us covered.\n",
        "\n",
        "## Loss Function\n",
        "\n",
        "In this case, we have **multivariate** linear regression, which is basically the same as simple linear regression, but in higher dimmensions. When we have multiple features, we are trying to fit a **hyperplane** that best describes our data.\n",
        "\n",
        "The loss function is the same, we're just dealing with numbers:\n",
        "\n",
        "$loss = L(\\textbf{W}) = \\frac{1}{n} \\sum_i(y-y')^2 =(Y- Y')^T(Y-Y')\\overset{\\mathrm{not}}{=}(Y-Y') ^2$ \n",
        "\n",
        "The weights still have a closed form, but it's calculated using matrices:\n",
        "\n",
        "$W = (X^TX)^{-1}X^TY$\n",
        "\n",
        "Good luck calculating that by hand."
      ]
    },
    {
      "metadata": {
        "id": "feK1MJAftUXY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implementation\n"
      ]
    },
    {
      "metadata": {
        "id": "YHY4jd9YqPxa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "transformer = PolynomialFeatures(degree=2)\n",
        "\n",
        "X_poly = transformer.fit_transform(seniority)\n",
        "X_poly[:20]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fSJ6PMPHscx0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Smooth."
      ]
    },
    {
      "metadata": {
        "id": "5l_9k67hseOi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = LinearRegression()\n",
        "model.fit(X_poly, salary)\n",
        "\n",
        "plt.plot(seniority, salary, 'o', markersize=5)\n",
        "\n",
        "\n",
        "def plot_polynomial(model, degree, start, end):\n",
        "  X = np.linspace(start, end, 1000).reshape(-1, 1)\n",
        "  transformed = PolynomialFeatures(degree=degree).fit_transform(X)\n",
        "  y = model.predict(transformed)\n",
        "  \n",
        "  plt.plot(X, y)\n",
        "\n",
        "plot_polynomial(model, 2, seniority.min(), seniority.max())\n",
        "\n",
        "plt.gcf().set_size_inches(14, 7)\n",
        "plt.xlabel(\"Seniority\")\n",
        "plt.ylabel(\"Salary\")\n",
        "plt.title(\"Employee Salary Evolution\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "egqiLAppt6JY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Looks much better! Let's see the score:"
      ]
    },
    {
      "metadata": {
        "id": "qbpAyDiFt-K7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.score(X_poly, salary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KYyPsJRVuLQP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The bigger the better amirite?"
      ]
    },
    {
      "metadata": {
        "id": "GejjlDrUqE0O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Coding Challenge\n",
        "\n",
        "Go wild. Try to fit polynomials of higher degrees. See what happens."
      ]
    },
    {
      "metadata": {
        "id": "yNmIAlkKt1t2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO fit higher degree polynomials and plot the results.\n",
        "\n",
        "# TODO check the score for each degree\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tx0KHRi3tmaD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dealing with overfitting\n",
        "\n",
        "Well, the score is higher right? Why is this wrong?\n",
        "\n",
        "The model [overfitted](https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76). Our model si **too** complex for our simple data. But how can we know for sure that this is the case?\n",
        "\n",
        "Overfiting can be a problem if the **training error** is much smaller than the **validation error**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "dBUvgeaTvw56",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://cdnpythonmachinelearning.azureedge.net/wp-content/uploads/2017/09/Overfitting.png?x31195)"
      ]
    },
    {
      "metadata": {
        "id": "uhNHZ9fEvzU9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We actually scored our model on the training data. Actually, that number tells us very little about the performance of our model. A more realistic measure is the test error.\n",
        "\n",
        "We can split our data intro training and test via sklearn's *train_test_split()* function."
      ]
    },
    {
      "metadata": {
        "id": "HxybPN9ixWCx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://cdn-images-1.medium.com/max/1600/1*-8_kogvwmL1H6ooN1A1tsQ.png =400x)"
      ]
    },
    {
      "metadata": {
        "id": "PnkjUfi8jL1-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(seniority, salary, test_size=0.3)\n",
        "\n",
        "# train on x_train / y_train\n",
        "\n",
        "features = PolynomialFeatures(degree=50)\n",
        "\n",
        "x_train_poly = features.fit_transform(x_train)\n",
        "model.fit(x_train_poly, y_train)\n",
        "\n",
        "training_error = model.score(x_train_poly, y_train)\n",
        "\n",
        "# validate on x_test, y_test\n",
        "\n",
        "test_error = model.score(features.fit_transform(x_test), y_test)\n",
        "\n",
        "\n",
        "print(\"Training score\", training_error)\n",
        "print(\"Test score\", test_error)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lm1WP_eByS6g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Looks like it's not doing so well this time. Test score is really low. A clear sign of overfitting.\n",
        "\n",
        "\n",
        "When training models, you should **always** keep some data for testing. This is done so that we can simulate a real world scenario where we don't have a label, and we can't find one."
      ]
    },
    {
      "metadata": {
        "id": "9h4QfYnROF9u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://scontent.fotp3-2.fna.fbcdn.net/v/t1.0-9/46742229_2149103865102836_4451942479448506368_n.jpg?_nc_cat=111&_nc_ht=scontent.fotp3-2.fna&oh=5abdaeff54b8986b128a076f7f1079a6&oe=5C6696FB =400x)"
      ]
    },
    {
      "metadata": {
        "id": "ECGFYwv26JvI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## K-Fold Cross Validation\n",
        "\n",
        "![](https://www.researchgate.net/profile/Juan_Buhagiar2/publication/322509110/figure/fig1/AS:583173118664704@1516050714606/An-example-of-a-10-fold-cross-validation-cro17.png)\n",
        "\n",
        "\n",
        "When we don't have enough data and / or we want a more robust evaluation of our model, we can us K-Fold Cross validation. We split our data into k equal-sized folds, and we train our model k times, each time leaving a fold for testing. The average accuracy of the folds is the final accuracy of the model.\n",
        "\n",
        "\n",
        "\n",
        "### Further reading\n",
        "- https://machinelearningmastery.com/k-fold-cross-validation/"
      ]
    },
    {
      "metadata": {
        "id": "AUf1Ko0768vT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "X = np.array([1, 2, 3, 4])\n",
        "y = np.array([10, 20, 30, 40]).reshape(-1, 1)\n",
        "\n",
        "print(X)\n",
        "print(y)\n",
        "\n",
        "kf = KFold(n_splits=4)\n",
        "kf.get_n_splits(X)\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "  print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "\n",
        "  X_train, X_test = X[train_index], X[test_index]\n",
        "  y_train, y_test = y[train_index], y[test_index]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u7mp7ySPzbQo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Ridge and Lasso Regression\n",
        "\n",
        "Sound fancy. It isn't. \n",
        "\n",
        "[Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) and [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) regression are regularization techniques for limiting the size of the weights in the model. Why are we interested in that? Smaller weights means that the model is robust to small changes in the data. It's a method for preventing overfitting for more complex models. Lasso can make some weights to be exactly 0.\n",
        "\n",
        "Ridge regression uses [L2 regularization](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c) on the loss function, while Lasso uses L1.\n",
        "\n",
        "\n",
        "Both have a parameter $\\alpha \\gt 0$ for controlling the power of regularization. \n",
        "\n",
        "### Ridge Loss function\n",
        "\n",
        "$loss = L_{ridge}(w) = \\frac{1}{n}\\sum_i(y' - y)^2 + \\alpha(\\Vert w_0\\Vert^2 + \\Vert w_1\\Vert^2)$\n",
        "\n",
        "### Lasso Loss function\n",
        "\n",
        "$loss = L_{lasso}(w) = \\frac{1}{n}\\sum_i(y' - y)^2 + \\alpha(\\Vert w_0\\Vert + \\Vert w_1\\Vert)$\n",
        "\n",
        "\n",
        "The added terms force the weights to be smaller.\n",
        "\n",
        "\n",
        "## Further Reading\n",
        "You can read more about these regularizations techniques here:\n",
        "- http://statweb.stanford.edu/~tibs/sta305files/Rudyregularization.pdf\n",
        "- https://courses.cs.washington.edu/courses/cse446/17wi/slides/ridgeregression.pdf\n",
        "- https://courses.cs.washington.edu/courses/cse599c1/13wi/slides/LARS-fusedlasso.pdf\n"
      ]
    },
    {
      "metadata": {
        "id": "3WLebuVZ1hAj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://i.stack.imgur.com/BBRXC.png)"
      ]
    },
    {
      "metadata": {
        "id": "vWE7iBc6qLO5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Coding Challenge\n",
        "\n",
        "Use Ridge and Lasso to fit a higher degree polynomial on the data. Careful for overfitting!"
      ]
    },
    {
      "metadata": {
        "id": "9mDRD7ZyqPbu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge, Lasso\n",
        "\n",
        "\n",
        "# TODO train Ridge and Lasso Regression on polynomial features\n",
        "# Hint: check the documentation\n",
        "# Hint: use train_test_split\n",
        "\n",
        "# TODO tweak the alpha parameter and see what happens\n",
        "\n",
        "# TODO plot the fitted polynomials\n",
        "\n",
        "\n",
        "# BONUS: evaluate your model using K-fold cross validation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2ahpqp8mpf5D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Coding Challenges: Multivariate Linear Regression"
      ]
    },
    {
      "metadata": {
        "id": "U5hJxS7tDBKN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Moore "
      ]
    },
    {
      "metadata": {
        "id": "lNHvhLN_DEg0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "file = urlopen('https://raw.githubusercontent.com/wallento/mooreandmore/master/raw_data.csv')\n",
        "data = pd.read_csv(file, delimiter=';', usecols=['Year', 'Transistors'])\n",
        "\n",
        "data.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sO5bPoI9INqD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# World Happiness Report"
      ]
    },
    {
      "metadata": {
        "id": "S4M38-fY2jAW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "file = urlopen('https://raw.githubusercontent.com/bucharestschoolofai/epoch_2/master/2017.csv')\n",
        "data = pd.read_csv(file, usecols=['Happiness.Score', 'Freedom', 'Economy..GDP.per.Capita.', 'Health..Life.Expectancy.'])\n",
        "\n",
        "data.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8KDUtIBDJkqS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Facebook"
      ]
    },
    {
      "metadata": {
        "id": "8VwMV8sL9d4I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "file = urlopen('https://raw.githubusercontent.com/bucharestschoolofai/epoch_2/master/dataset_Facebook.csv')\n",
        "data = pd.read_csv(file, delimiter=';', usecols=['Total Interactions', 'Post Month', 'Post Weekday', 'Post Hour', 'Page total likes'])\n",
        "\n",
        "data.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PtTuhLh8O0N0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://imgs.xkcd.com/comics/correlation.png)"
      ]
    },
    {
      "metadata": {
        "id": "-rUVouqH2RjW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}